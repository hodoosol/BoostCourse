"""
2021.06.15 수요일

[AI Tech Pre-course] 인공지능(AI) 기초 다지기
Chapter3. 기초 수학 첫걸음

4. 경사하강법 (매운맛)

"""


"""
* 선형회귀분석 복습
np.linalg.pinv로 데이터를 선형모델로 해석하는 선형회귀식을 찾을 수 있다.

1. 경사하강법으로 선형회귀 계수 구하기
- 선형회귀의 목적식은 l2 norm이다.
- 선형회귀의 목적식은 ||y - XB||2 이고, 이를 최소화하는 B를 찾아야 하므로
  다음과 같은 그래디언트 벡터를 구해야 한다.
  ▽B||y - XB||x = (aB1||y - XB||x, ..., aBd||y - XB||x)
- 즉, l2 norm을 최소화하는 beta를 찾아야 한다.

* 경사하강법 기반 선형회귀 알고리즘
Input : X, y, lr, T(학습 횟수)
Output : beta
# norm : L2-노름을 계산하는 함수
 
for t in range(T) :
    error = y - x @ beta
    grad = - transpose(X) @ error
    beta = beta - lr * grad

-> 이제 이 알고리즘으로 역행렬을 이요하지 않고 회귀계수를 계산할 수 있다.

** 
경사하강법 알고리즘에서는 학습률과 학습횟수가 중요한 hyperparameter이다.

이론적으로 경사하강법은 미분 가능하고 볼록(convex)한 함수에 대해서는
적절한 학습률과 학습 횟수를 선택했을 때 수렴이 보장되어 있다.
(볼록한 함수는 그래디언트 벡터가 항상 최소점을 향한다.)

특히 선형회귀의 경우 목적식 ||y - XB||2 는 회귀계수 B에 대해
볼록함수이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장된다.

하지만 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로
수렴이 항상 보장되지는 않는다.
특히 딥러닝을 사용하는 경우 목적식은 대부분 볼록함수가 아니다.


"""



"""
2. SGD : 확률적 경사하강법 (stochastic gradient descent)
- 확률적 경사하강법은 모든 데이터를 사용해서 업데이트하는 대신에
  데이터 1개, 또는 일부를 활용하여 업데이트 한다.
     데이터 1개 사용            : sgd
     데이터 일부(미니배치 사용)   : 미니배치 sgd (대부분 이것 사용)
     
- 볼록이 아닌 non-convex 목적식은 sgd를 통해 최적화할 수 있다.
- SGD라고 해서 만능은 아니지만 딥러닝의 경우,
  sgd가 경사하강법보다 실질적으로 더 낫다고 검증되었다.
  
- 그래디언트 계산시, 데이터의 일부만을 사용하기 때문에
  모든 데이터를 사용한 그래디언트 계산과 절대 같을 수는 없지만 유사함이 보장됨
  
- sgd는 데이터의 일부를 가지고 파라미터를 업데이트하기 때문에
  연산자원을 좀 더 효율적으로 활용하는데 도움이 된다.
    전체 데이터(X, y)가 아닌 미니배치(X(b), y(b))를 쓰므로
    연산량이 b / n으로 감소한다.
  

* SGD의 원리 : 미니배치 연산
- 경사하강법은 전체 데이터 D = (X, y)를 가지고 
  목적식의 그래디언트 벡터인 ▽ΘL(D, Θ)를 계산한다. 
  L(D, Θ) == 전체 데이터 D와 파라미터 Θ로 측정한 목적식, Θ == theta

- sgd는 미니배치 D(b) = (X(b), y(b))를 가지고 그래디언트 벡터를 근사해서 계산
  미니배치는 확률적으로 선택하므로 매번 다른 미니배치를 사용하기 때문에
  목적식의 모양(곡선 모양)이 바뀌게 된다.

- sgd는 볼록이 아닌 목적식에서도 사용 가능하므로 경사하강법보다 머신러닝 학습에 더 효율적이다.
  실제로는 왔다갔다하는 모양새로 움직이게 된다. 그래도 최소점을 향한다.
  각각의 계산 속도는 경사하강법보다 빠르다.
  
- 미니배치의 사이즈가 지나치게 작으면 경사하강법보다 느려질 수 있다.

- 중요한 hyperparameter : lr, T(학습 횟수), mini-batch size

- 아주 아주 큰 사이즈의 이미지 데이터를 일반적인 경사하강법처럼 모두 한번에 업로드하면
  메모리가 부족하여 Out-of-memory가 발생한다.
  but, sgd를 이용하여 미니배치로 쪼갠 데이터를 업로드하면 빠른 연산이 가능하다.
    GPU에서 행렬 연산과 모델 파라미터를 업데이트 하는 동안 CPU는 전처리와 GPU에서 업로드할 데이터를 준비한다.
  

"""




